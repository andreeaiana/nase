# @package _global_

# to execute this experiment run:
# python train.py experiment=train_nase_parallel

defaults:
  - override /data: datamodule.yaml
  - override /model: nase.yaml
  - override /callbacks: default.yaml
  - override /logger: wandb.yaml
  - override /trainer: gpu.yaml

tags: ["pretrain", "nase", "mt_after_dae"]


data:
  parallel_data: True
  polynews_dataset_name: "aiana94/polynews-parallel"
  tokenizer: 
    _target_: transformers.AutoTokenizer.from_pretrained
    _recursive_: False
    pretrained_model_name_or_path: "trained_models/nase_dae" 
    use_fast: True
 
trainer:
  max_steps: 500000
  val_check_interval: 5000 

callbacks:
  model_checkpoint:
    every_n_train_steps: ${trainer.val_check_interval}

seed: 42

logger:
  wandb:
    name: "nase_mt_afer_dae_ms=${trainer.max_steps}_val=${trainer.val_check_interval}_bs=${data.train_batch_size}_lr=${model.optimizer.lr}_seed=${seed}"
    tags: ${tags}
    group: "pretrain"
